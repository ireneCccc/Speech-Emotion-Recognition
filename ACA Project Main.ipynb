{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACA Project: Speech Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "import os, time, csv, datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Reading emotion #0 in TESS...\n",
      "Reading emotion #1 in TESS...\n",
      "Reading emotion #2 in TESS...\n",
      "Reading emotion #3 in TESS...\n",
      "Reading emotion #4 in TESS...\n",
      "Reading emotion #5 in TESS...\n",
      "Reading emotion #6 in TESS...\n",
      "Reading emotion #0 in RAVDESS...\n",
      "Reading emotion #1 in RAVDESS...\n",
      "Reading emotion #2 in RAVDESS...\n",
      "Reading emotion #3 in RAVDESS...\n",
      "Reading emotion #4 in RAVDESS...\n",
      "Reading emotion #5 in RAVDESS...\n",
      "Reading emotion #6 in RAVDESS...\n",
      "Finished. (22 s in total)\n"
     ]
    }
   ],
   "source": [
    "## read audio wav from dataset TESS and RAVDESS\n",
    "\n",
    "parameters = [7, -1, 1024, 768, 80, 300, 8000, 50]\n",
    "[emo_read_num, file_read_num, win_size, hop_size, min_freq, max_fund_freq, max_freq, mfcc_size] = [int(x) for x in parameters]\n",
    "\n",
    "TESS_trim = 0.62\n",
    "RAVDESS_trim = 0.26\n",
    "magic = 43195\n",
    "\n",
    "time_very_start = time.time()\n",
    "print('Start')\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "nowdate = datetime.datetime.now()\n",
    "savename = 'feat_'  + str(win_size) + 'win_' + \\\n",
    "           '[' + str(nowdate.month).zfill(2) + str(nowdate.day).zfill(2) + '-' + \\\n",
    "           str(nowdate.hour).zfill(2) + str(nowdate.minute).zfill(2) + '].csv'\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "for dataset in ['TESS', 'RAVDESS']:\n",
    "    dataset_dir = os.path.join(parent_dir, 'project', dataset)\n",
    "    for emotion in range(7):\n",
    "        if emotion >= emo_read_num:\n",
    "            break\n",
    "        time_start = time.time()\n",
    "        print('Reading emotion #' + str(emotion) + ' in ' + dataset + '...')\n",
    "        emotion_dir = os.path.join(dataset_dir, str(emotion))\n",
    "        file_count = 0\n",
    "        file_list = os.listdir(emotion_dir)\n",
    "        for file in file_list:\n",
    "            if file_read_num != -1 and file_count >= file_read_num:\n",
    "                break\n",
    "            if (not file.endswith('.wav')) or file[0] == '.':\n",
    "                continue\n",
    "            fs, x = wavfile.read(os.path.join(emotion_dir, file))\n",
    "            \n",
    "            if len(x) >= magic:\n",
    "                if dataset == 'TESS':\n",
    "                    x = x[int((len(x) - magic) / 2):][:magic]\n",
    "                else:\n",
    "                    x = x[-magic:]\n",
    "            else: # add zeros at end\n",
    "                x = np.concatenate((x, [0] * (magic - len(x))))\n",
    "\n",
    "            x = x / 32768 # convert 16-bit PCM to [-1, 1]\n",
    "\n",
    "            s = librosa.feature.melspectrogram(y=x, sr=fs, n_fft=win_size, hop_length=hop_size)\n",
    "            mfcc = librosa.feature.mfcc(S=librosa.power_to_db(s), sr=fs, n_mfcc=mfcc_size)\n",
    "\n",
    "            rms = librosa.feature.rmse(y=x, frame_length=win_size, hop_length=hop_size)\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=x, frame_length=win_size, hop_length=hop_size)\n",
    "            centroid = librosa.feature.spectral_centroid(y=x, sr=fs, n_fft=win_size, hop_length=hop_size)\n",
    "            \n",
    "            # pitch\n",
    "            min_lag = int(fs / max_fund_freq)\n",
    "            max_lag = int(fs / min_freq)\n",
    "            L = range(min_lag, max_lag + 1)\n",
    "            spec = librosa.core.stft(x, n_fft=win_size, hop_length=hop_size, win_length=win_size)\n",
    "            dividend = np.transpose([np.real(np.fft.ifft(row)) for row in (np.absolute(spec) ** 2).transpose()])\n",
    "            divisor = np.transpose([win_size - lag + 1 for lag in L])\n",
    "            acf = dividend[L] / divisor[:, None]\n",
    "            i_max = np.argmax(acf, axis=0)\n",
    "            pitch = fs / (i_max - 1 + min_lag)\n",
    "            \n",
    "            if len(set([len(mfcc[0]), len(rms[0]), len(zcr[0]), len(centroid[0]), len(pitch)])) != 1:\n",
    "                print('  Error: File ' + file + ' has different numbers of windows among different features!')\n",
    "                continue\n",
    "\n",
    "            if dataset == 'TESS':\n",
    "                gender = np.vstack(([0] * len(rms[0]), [1] * len(rms[0])))\n",
    "            else:\n",
    "                if int(file[19]) % 2 == 0:\n",
    "                    gender = np.vstack(([0] * len(rms[0]), [1] * len(rms[0]))) # female\n",
    "                else:\n",
    "                    gender = np.vstack(([1] * len(rms[0]), [0] * len(rms[0]))) # male\n",
    "\n",
    "            # vertically concatenate features of all windows\n",
    "            concat = np.vstack((mfcc, rms, zcr, centroid, pitch, gender))\n",
    "            features.append(concat)\n",
    "            labels.append(emotion)\n",
    "            file_count += 1\n",
    "#         print('    ' + str(file_count) + ' files feature extracted. (' + str(int(time.time() - time_start)) + ' s)')\n",
    "\n",
    "print('Finished. (' + str(int(time.time() - time_very_start)) + ' s in total)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 56, 57)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "fea = np.array(features)\n",
    "lab = np.array(labels)\n",
    "print(fea.shape)\n",
    "print(lab.shape)\n",
    "rowdim = fea.shape[0]\n",
    "ydim = fea.shape[1]\n",
    "xdim = fea.shape[2]\n",
    "# print(ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## original used to read in data, not use anymore\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # read data from xls file \n",
    "# df = pd.read_csv('features.csv', header = None, na_values = '?', index_col = None)\n",
    "\n",
    "# # filenames = ['features2.csv', 'features3.csv', 'features4.csv', 'features5.csv', 'features6.csv','test1.csv','test2.csv']\n",
    "# # for filename in filenames:\n",
    "# #     df1 = pd.read_csv(filename, header = None, na_values = '0', index_col = None)\n",
    "# #     df = pd.concat([df,df1], ignore_index = True)\n",
    "# data_column = np.shape(df)[1]\n",
    "# data_row = np.shape(df)[0]\n",
    "# print(np.shape(df))\n",
    "# df.head(5)\n",
    "\n",
    "# # get the value \n",
    "# y_raw = np.array(df[data_column-2])\n",
    "\n",
    "# # list of 1D-features in the order of MFCC, Energy, Pitch\n",
    "# X_raw = np.array(pd.DataFrame(df, columns = df.columns[0:(data_column-2)]))\n",
    "\n",
    "# print(np.shape(X_raw))\n",
    "# print(np.shape(y_raw))\n",
    "# # print(X_raw)\n",
    "# # print(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 3192)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "X_raw = fea.reshape(rowdim, ydim*xdim)\n",
    "y_raw = lab\n",
    "print(X_raw.shape)\n",
    "print(y_raw.shape)\n",
    "\n",
    "# normalize\n",
    "X = preprocessing.scale(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 3192)\n",
      "(2094,)\n",
      "(524, 3192)\n",
      "(524,)\n"
     ]
    }
   ],
   "source": [
    "# randomly split data into train(80%) and test(20%) set\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classifier 1: SVM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.851145038168\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# used for finding best parameters(C and gamma)\n",
    "# for i in range(1,10):\n",
    "#     svc = svm.SVC(kernel = 'rbf', C = i/10, gamma = 'auto', verbose = 10)\n",
    "#     svc.fit(Xtr,ytr)\n",
    "#     yhat_ts = svc.predict(Xts)\n",
    "#     acc = accuracy_score(yhat_ts,yts)\n",
    "#     print(acc)\n",
    "        \n",
    "svc = svm.SVC(kernel = 'rbf', C = 6, gamma = 'auto', verbose = 10)\n",
    "svc.fit(Xtr,ytr)\n",
    "yhat_ts = svc.predict(Xts)\n",
    "acc = accuracy_score(yhat_ts,yts)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9429  0.      0.      0.      0.0128  0.023   0.0135]\n",
      " [ 0.0143  0.8846  0.0125  0.      0.0513  0.      0.    ]\n",
      " [ 0.0143  0.      0.8375  0.0482  0.0256  0.023   0.0541]\n",
      " [ 0.0286  0.0192  0.0375  0.8072  0.0256  0.0345  0.0676]\n",
      " [ 0.0143  0.0577  0.025   0.0964  0.8205  0.      0.    ]\n",
      " [ 0.0571  0.      0.025   0.012   0.      0.9195  0.    ]\n",
      " [ 0.0429  0.0192  0.05    0.0241  0.0641  0.0345  0.7568]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "size0 = np.shape(np.where(yts==0))[1]\n",
    "size1 = np.shape(np.where(yts==1))[1]\n",
    "size2 = np.shape(np.where(yts==2))[1]\n",
    "size3 = np.shape(np.where(yts==3))[1]\n",
    "size4 = np.shape(np.where(yts==4))[1]\n",
    "size5 = np.shape(np.where(yts==5))[1]\n",
    "size6 = np.shape(np.where(yts==6))[1]\n",
    "size = np.array([1/size0, 1/size1, 1/size2, 1/size3, 1/size4, 1/size5, 1/size6])\n",
    "\n",
    "dim = yts.shape[0]\n",
    "# print(dim)\n",
    "C = confusion_matrix(yts, yhat_ts, labels=None, sample_weight=None)\n",
    "# print(C)\n",
    "C_normalized = size*C\n",
    "print(np.array_str(C_normalized, precision=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Convolution2D, MaxPooling2D, Activation, concatenate, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use pre-trained deep learning network vgg16\n",
    "# not useful in this task, bad performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # pre-trained deep learning network vgg16\n",
    "# Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "# Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "# print(Xtr.shape)\n",
    "# print(Xts.shape)\n",
    "# Xtr_1 = []\n",
    "# Xts_1 = []\n",
    "# Xtr_1 = Xtr.repeat(3, axis=3)\n",
    "# Xts_1 = Xts.repeat(3, axis=3)\n",
    "# print(Xtr_1.shape)\n",
    "# print(Xts_1.shape)\n",
    "\n",
    "# pre_trained = 'vgg16'\n",
    "\n",
    "# # Load appropriate packages\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import decode_predictions, preprocess_input    \n",
    "\n",
    "# input_shape = (ydim,xdim,3)\n",
    "# base_model = applications.VGG16(weights='imagenet', include_top = False, input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# for layers in base_model.layers:\n",
    "#     model.add(layers)\n",
    "    \n",
    "# for layers in model.layers:\n",
    "#     layers.trainable = False\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256,activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation = 'sigmoid'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nepochs = 5  # Number of epochs\n",
    "\n",
    "# # Call the fit function\n",
    "# model.fit(Xtr_1, ytr, batch_size=32, epochs=nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yhat = model.predict(Xts_1)\n",
    "# yhat = np.argmax(yhat, axis=1)\n",
    "# print(yhat.shape)\n",
    "# yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy_score(yts, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam)\n",
    "# model.fit(x, y, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "#                        verbose=2, validation_data=(xt, yt), show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important notice\n",
    " \n",
    " There are 3 models in the code below, one time can only run one model, uncomment/comment use '#' as you want.\n",
    " Keyboard shortcut for uncomment/comment: 'command' + '/'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classifier 2: self-defined CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 56, 57)\n"
     ]
    }
   ],
   "source": [
    "#kares package use Tensorflow as backend\n",
    "Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "\n",
    "#change the features to improve accurancy\n",
    "rdim = Xtr.shape[0]\n",
    "sdim = Xts.shape[0]\n",
    "Xtr = Xtr.reshape(rdim, ydim, xdim)\n",
    "Xts = Xts.reshape(sdim, ydim, xdim)\n",
    "\n",
    "# feature selection method\n",
    "# Xtr = np.delete(Xtr, [19,20], 1)\n",
    "# Xts = np.delete(Xts, [19,20], 1)\n",
    "print(Xtr.shape)\n",
    "ydim = ydim - 0\n",
    "\n",
    "# one hot encoded\n",
    "ytr_reshape = ytr.reshape(-1, 1)\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "ytr_hot = encoder.fit_transform(ytr_reshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CNN model with 2 convolution layer\n",
    "\n",
    "Xtr = Xtr.reshape(rdim, ydim, xdim, 1)\n",
    "Xts = Xts.reshape(sdim, ydim, xdim, 1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 32 \n",
    "in_shape = (ydim, xdim, 1)\n",
    "\n",
    "# normalize for each batch \n",
    "model.add(BatchNormalization(input_shape=in_shape))\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (1,9), input_shape=in_shape))\n",
    "model.add(MaxPooling2D(pool_size=(1,3))) \n",
    "model.add(Dropout(0.1)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, (1,9)))\n",
    "model.add(MaxPooling2D(pool_size=(1,3)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten()) \n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(7,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CNN model with 2 parallel convolution layer\n",
    "\n",
    "# Xtr = Xtr.reshape(rdim, ydim, xdim, 1)\n",
    "# Xts = Xts.reshape(sdim, ydim, xdim, 1)\n",
    "\n",
    "# conv_filters = 16\n",
    "# in_shape = (ydim, xdim, 1)\n",
    "# input = Input(in_shape)\n",
    "\n",
    "# # parallel convolution layer on two dimensions\n",
    "# conv_layer1 = Convolution2D(conv_filters, (9,1), activation='relu')(input)  # a vertical filter\n",
    "# conv_layer2 = Convolution2D(conv_filters, (1,9), activation='relu')(input)  # a horizontal filter\n",
    "# # conv_layer3 = Convolution1D(n_filters, (1,3), activation='relu')(input[2])  # a horizontal filter\n",
    "\n",
    "# # pooling layers\n",
    "# maxpool1 = MaxPooling2D(pool_size=(1,3))(conv_layer1)\n",
    "# maxpool2 = MaxPooling2D(pool_size=(3,1))(conv_layer2) \n",
    "\n",
    "# # dropout layers\n",
    "# maxpool1 = Dropout(0.1)(maxpool1)\n",
    "# maxpool2 = Dropout(0.1)(maxpool2)\n",
    "\n",
    "# # conv_layer3 = Convolution2D(n_filters, (9,1), activation='relu')(maxpool1)\n",
    "# # conv_layer4 = Convolution2D(n_filters, (1,9), activation='relu')(maxpool2)\n",
    "# # maxpool3 = MaxPooling2D(pool_size=(1,3))(conv_layer3)\n",
    "# # maxpool4 = MaxPooling2D(pool_size=(3,1))(conv_layer4) # used 4,1 first\n",
    "# # maxpool3 = Dropout(0.25)(conv_layer3)\n",
    "# # maxpool4 = Dropout(0.25)(conv_layer4)\n",
    "# # maxpool4 = Dropout(0.25)(maxpool4)\n",
    "\n",
    "# # flatten layers\n",
    "# poolflat1 = Flatten()(maxpool1)\n",
    "# poolflat2 = Flatten()(maxpool2)\n",
    "\n",
    "# # Merge the 2 parallel pipelines\n",
    "# merged = concatenate([poolflat1, poolflat2],1)\n",
    "\n",
    "# full = Dense(256, activation='sigmoid')(merged)\n",
    "# output_layer = Dense(7, activation='softmax')(full)\n",
    "\n",
    "# # create the model\n",
    "# model = Model(input=input, output=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 3: LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(512, return_sequences=True, input_shape=(ydim, xdim)))\n",
    "# model.add(Activation('relu'))\n",
    "#   # model.add(LSTM(512, return_sequences=True))\n",
    "#   # model.add(Activation('tanh'))\n",
    "# model.add(LSTM(256, return_sequences=False))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 56, 57, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 56, 49, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 8, 32)         9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               917760    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 929,131\n",
      "Trainable params: 929,129\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy' \n",
    "# loss = 'sparse_categorical_crossentropy' \n",
    "\n",
    "# use adam optimizer\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1884 samples, validate on 210 samples\n",
      "Epoch 1/10\n",
      "1884/1884 [==============================] - 4s - loss: 1.0421 - acc: 0.6274 - val_loss: 0.7942 - val_acc: 0.7190\n",
      "Epoch 2/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.5504 - acc: 0.8132 - val_loss: 0.6630 - val_acc: 0.7524\n",
      "Epoch 3/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.3478 - acc: 0.8737 - val_loss: 0.5511 - val_acc: 0.8143\n",
      "Epoch 4/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.2170 - acc: 0.9352 - val_loss: 0.5053 - val_acc: 0.8095\n",
      "Epoch 5/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.1431 - acc: 0.9613 - val_loss: 0.5006 - val_acc: 0.8333\n",
      "Epoch 6/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.0913 - acc: 0.9846 - val_loss: 0.4886 - val_acc: 0.8476\n",
      "Epoch 7/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.0553 - acc: 0.9936 - val_loss: 0.4967 - val_acc: 0.8238\n",
      "Epoch 8/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.0373 - acc: 0.9984 - val_loss: 0.4857 - val_acc: 0.8381\n",
      "Epoch 9/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.0233 - acc: 0.9989 - val_loss: 0.4819 - val_acc: 0.8476\n",
      "Epoch 10/10\n",
      "1884/1884 [==============================] - 4s - loss: 0.0169 - acc: 1.0000 - val_loss: 0.4810 - val_acc: 0.8381\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# epochs numbers to train(common one)\n",
    "nepochs = 10 # for self-build CNN\n",
    "# nepochs = 20 # for LSTM only\n",
    "\n",
    "# for training we need the \"1 hot encoded\" numeric classes of the ground truth\n",
    "validation_percent = 0.1\n",
    "History = model.fit(Xtr, ytr_hot, validation_split=validation_percent, batch_size=32, epochs=nepochs)\n",
    "\n",
    "# keep history of accuracies on training set\n",
    "# append to previous history in case multiple times are excuted\n",
    "if history is None:\n",
    "    history = History.history\n",
    "else:\n",
    "    for key in History.history.keys():\n",
    "        history[key].extend(History.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yts_pred = model.predict(Xts)\n",
    "yts_pred = np.argmax(yts_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yts_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8492366412213741"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yts, yts_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9286  0.      0.      0.012   0.      0.0115  0.0405]\n",
      " [ 0.0143  0.9423  0.      0.      0.0256  0.      0.    ]\n",
      " [ 0.0143  0.      0.8125  0.0602  0.      0.046   0.0676]\n",
      " [ 0.      0.0769  0.025   0.8072  0.0128  0.0345  0.0811]\n",
      " [ 0.0429  0.1154  0.0375  0.0482  0.7564  0.023   0.0135]\n",
      " [ 0.0286  0.      0.0125  0.      0.      0.9655  0.    ]\n",
      " [ 0.0714  0.0385  0.025   0.012   0.0513  0.046   0.7568]]\n"
     ]
    }
   ],
   "source": [
    "size0 = np.shape(np.where(yts==0))[1]\n",
    "size1 = np.shape(np.where(yts==1))[1]\n",
    "size2 = np.shape(np.where(yts==2))[1]\n",
    "size3 = np.shape(np.where(yts==3))[1]\n",
    "size4 = np.shape(np.where(yts==4))[1]\n",
    "size5 = np.shape(np.where(yts==5))[1]\n",
    "size6 = np.shape(np.where(yts==6))[1]\n",
    "size = np.array([1/size0, 1/size1, 1/size2, 1/size3, 1/size4, 1/size5, 1/size6])\n",
    "\n",
    "dim = yts.shape[0]\n",
    "# print(dim)\n",
    "C = confusion_matrix(yts, yts_pred, labels=None, sample_weight=None)\n",
    "# print(C)\n",
    "C_normalized = size*C\n",
    "print(np.array_str(C_normalized, precision=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
