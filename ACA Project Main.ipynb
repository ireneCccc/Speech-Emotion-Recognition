{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ACA Project: Speech Emotion Detection\n",
    "    \n",
    "we used data mainly from two dataset (TESS and RAVDESS), and transform them into the consist format (56x85) for each audio wav, and reval them into 1D features to save in the csv file for further read. The source code for transformation part is in file ... the last two column data in csv file is the class and dataset label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Reading emotion #0 in TESS...\n",
      "    196 files feature extracted. (1 s)\n",
      "Reading emotion #1 in TESS...\n",
      "    196 files feature extracted. (1 s)\n",
      "Reading emotion #2 in TESS...\n",
      "    196 files feature extracted. (1 s)\n",
      "Reading emotion #3 in TESS...\n",
      "    196 files feature extracted. (1 s)\n",
      "Reading emotion #4 in TESS...\n",
      "    196 files feature extracted. (1 s)\n",
      "Reading emotion #5 in TESS...\n",
      "    195 files feature extracted. (1 s)\n",
      "Reading emotion #6 in TESS...\n",
      "    195 files feature extracted. (1 s)\n",
      "Reading emotion #0 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Reading emotion #1 in RAVDESS...\n",
      "    96 files feature extracted. (0 s)\n",
      "Reading emotion #2 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Reading emotion #3 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Reading emotion #4 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Reading emotion #5 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Reading emotion #6 in RAVDESS...\n",
      "    192 files feature extracted. (1 s)\n",
      "Finished. (22 s in total)\n"
     ]
    }
   ],
   "source": [
    "# read audio wav from dataset TESS and RAVDESS\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "import os, time, csv, datetime\n",
    "\n",
    "# part = 'test'\n",
    "parameters = [7, -1, 1024, 768, 80, 300, 8000, 50]\n",
    "[emo_read_num, file_read_num, win_size, hop_size, min_freq, max_fund_freq, max_freq, mfcc_size] = [int(x) for x in parameters]\n",
    "\n",
    "TESS_trim = 0.62\n",
    "RAVDESS_trim = 0.26\n",
    "magic = 43195\n",
    "\n",
    "time_very_start = time.time()\n",
    "print('Start')\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "nowdate = datetime.datetime.now()\n",
    "savename = 'feat_'  + str(win_size) + 'win_' + \\\n",
    "           '[' + str(nowdate.month).zfill(2) + str(nowdate.day).zfill(2) + '-' + \\\n",
    "           str(nowdate.hour).zfill(2) + str(nowdate.minute).zfill(2) + '].csv'\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "for dataset in ['TESS', 'RAVDESS']:\n",
    "    dataset_dir = os.path.join(parent_dir, 'project', dataset)\n",
    "    for emotion in range(7):\n",
    "        if emotion >= emo_read_num:\n",
    "            break\n",
    "        time_start = time.time()\n",
    "        print('Reading emotion #' + str(emotion) + ' in ' + dataset + '...')\n",
    "        emotion_dir = os.path.join(dataset_dir, str(emotion))\n",
    "        file_count = 0\n",
    "        file_list = os.listdir(emotion_dir)\n",
    "        for file in file_list:\n",
    "            if file_read_num != -1 and file_count >= file_read_num:\n",
    "                break\n",
    "            if (not file.endswith('.wav')) or file[0] == '.':\n",
    "                continue\n",
    "            fs, x = wavfile.read(os.path.join(emotion_dir, file))\n",
    "            \n",
    "            if len(x) >= magic:\n",
    "                if dataset == 'TESS':\n",
    "                    x = x[int((len(x) - magic) / 2):][:magic]\n",
    "                else:\n",
    "                    x = x[-magic:]\n",
    "            else: # add zeros at end\n",
    "                x = np.concatenate((x, [0] * (magic - len(x))))\n",
    "\n",
    "            x = x / 32768 # convert 16-bit PCM to [-1, 1]\n",
    "\n",
    "            s = librosa.feature.melspectrogram(y=x, sr=fs, n_fft=win_size, hop_length=hop_size)\n",
    "            mfcc = librosa.feature.mfcc(S=librosa.power_to_db(s), sr=fs, n_mfcc=mfcc_size)\n",
    "\n",
    "            rms = librosa.feature.rmse(y=x, frame_length=win_size, hop_length=hop_size)\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=x, frame_length=win_size, hop_length=hop_size)\n",
    "            centroid = librosa.feature.spectral_centroid(y=x, sr=fs, n_fft=win_size, hop_length=hop_size)\n",
    "            \n",
    "            # pitch\n",
    "            min_lag = int(fs / max_fund_freq)\n",
    "            max_lag = int(fs / min_freq)\n",
    "            L = range(min_lag, max_lag + 1)\n",
    "            spec = librosa.core.stft(x, n_fft=win_size, hop_length=hop_size, win_length=win_size)\n",
    "            dividend = np.transpose([np.real(np.fft.ifft(row)) for row in (np.absolute(spec) ** 2).transpose()])\n",
    "            divisor = np.transpose([win_size - lag + 1 for lag in L])\n",
    "            acf = dividend[L] / divisor[:, None]\n",
    "            i_max = np.argmax(acf, axis=0)\n",
    "            pitch = fs / (i_max - 1 + min_lag)\n",
    "            \n",
    "            if len(set([len(mfcc[0]), len(rms[0]), len(zcr[0]), len(centroid[0]), len(pitch)])) != 1:\n",
    "                print('  Error: File ' + file + ' has different numbers of windows among different features!')\n",
    "                continue\n",
    "\n",
    "            if dataset == 'TESS':\n",
    "                gender = np.vstack(([0] * len(rms[0]), [1] * len(rms[0])))\n",
    "            else:\n",
    "                if int(file[19]) % 2 == 0:\n",
    "                    gender = np.vstack(([0] * len(rms[0]), [1] * len(rms[0]))) # female\n",
    "                else:\n",
    "                    gender = np.vstack(([1] * len(rms[0]), [0] * len(rms[0]))) # male\n",
    "\n",
    "            # vertically concatenate features of all windows\n",
    "            concat = np.vstack((mfcc, rms, zcr, centroid, pitch, gender))\n",
    "            features.append(concat)\n",
    "            labels.append(emotion)\n",
    "            file_count += 1\n",
    "        print('    ' + str(file_count) + ' files feature extracted. (' + str(int(time.time() - time_start)) + ' s)')\n",
    "\n",
    "print('Finished. (' + str(int(time.time() - time_very_start)) + ' s in total)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 56, 57)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "fea = np.array(features)\n",
    "lab = np.array(labels)\n",
    "print(fea.shape)\n",
    "print(lab.shape)\n",
    "rowdim = fea.shape[0]\n",
    "ydim = fea.shape[1]\n",
    "xdim = fea.shape[2]\n",
    "# print(ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # read data from xls file \n",
    "# df = pd.read_csv('features.csv', header = None, na_values = '?', index_col = None)\n",
    "\n",
    "# # filenames = ['features2.csv', 'features3.csv', 'features4.csv', 'features5.csv', 'features6.csv','test1.csv','test2.csv']\n",
    "# # for filename in filenames:\n",
    "# #     df1 = pd.read_csv(filename, header = None, na_values = '0', index_col = None)\n",
    "# #     df = pd.concat([df,df1], ignore_index = True)\n",
    "# data_column = np.shape(df)[1]\n",
    "# data_row = np.shape(df)[0]\n",
    "# print(np.shape(df))\n",
    "# df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the value \n",
    "# y_raw = np.array(df[data_column-2])\n",
    "\n",
    "# # list of 1D-features in the order of MFCC, Energy, Pitch\n",
    "# X_raw = np.array(pd.DataFrame(df, columns = df.columns[0:(data_column-2)]))\n",
    "\n",
    "# print(np.shape(X_raw))\n",
    "# print(np.shape(y_raw))\n",
    "# # print(X_raw)\n",
    "# # print(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 3192)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "X_raw = fea.reshape(rowdim, ydim*xdim)\n",
    "y_raw = lab\n",
    "print(X_raw.shape)\n",
    "print(y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for one hot coding \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# y_reshape = y_raw.reshape(-1, 1)\n",
    "# encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "# yhot = encoder.fit_transform(y_reshape)\n",
    "# print(yhot)\n",
    "# print(yhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "# normalize\n",
    "\n",
    "X = preprocessing.scale(X_raw)\n",
    "# print(X.shape)\n",
    "# scaler = preprocessing.standardScaler().fit(X_raw)\n",
    "# X = scaler.transform(X_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 3192)\n",
      "(2094,)\n",
      "(524, 3192)\n",
      "(524,)\n",
      "Number of files in each category in TRAIN set:\n",
      "318\n",
      "240\n",
      "308\n",
      "305\n",
      "310\n",
      "300\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "\n",
    "cnt = Counter(ytr)\n",
    "print(\"Number of files in each category in TRAIN set:\")\n",
    "for k in sorted(cnt.keys()):\n",
    "    print(cnt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use SVM model to predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.81679389313\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for i in range(1,10):\n",
    "#     svc = svm.SVC(kernel = 'rbf', C = i/10, gamma = 'auto', verbose = 10)\n",
    "#     svc.fit(Xtr,ytr)\n",
    "#     yhat_ts = svc.predict(Xts)\n",
    "#     acc = accuracy_score(yhat_ts,yts)\n",
    "#     print(acc)\n",
    "        \n",
    "svc = svm.SVC(kernel = 'rbf', C = 6, gamma = 'auto', verbose = 10)\n",
    "svc.fit(Xtr,ytr)\n",
    "yhat_ts = svc.predict(Xts)\n",
    "acc = accuracy_score(yhat_ts,yts)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yhat_ts = svc.predict(Xts)\n",
    "# acc = np.mean(yhat_ts == yts)\n",
    "# print('Accuaracy = {0:f}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Convolution2D, MaxPooling2D, Activation, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to use pre-trained deep learning network vgg16\n",
    "# Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "# Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "# print(Xtr.shape)\n",
    "# print(Xts.shape)\n",
    "# Xtr_1 = []\n",
    "# Xts_1 = []\n",
    "# Xtr_1 = Xtr.repeat(3, axis=3)\n",
    "# Xts_1 = Xts.repeat(3, axis=3)\n",
    "# print(Xtr_1.shape)\n",
    "# print(Xts_1.shape)\n",
    "\n",
    "# pre_trained = 'vgg16'\n",
    "\n",
    "# # Load appropriate packages\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import decode_predictions, preprocess_input    \n",
    "\n",
    "# input_shape = (ydim,xdim,3)\n",
    "# base_model = applications.VGG16(weights='imagenet', include_top = False, input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# for layers in base_model.layers:\n",
    "#     model.add(layers)\n",
    "    \n",
    "# for layers in model.layers:\n",
    "#     layers.trainable = False\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256,activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation = 'sigmoid'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepochs = 5  # Number of epochs\n",
    "\n",
    "# # Call the fit function\n",
    "# model.fit(Xtr_1, ytr, batch_size=32, epochs=nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model.predict(Xts_1)\n",
    "# yhat = np.argmax(yhat, axis=1)\n",
    "# print(yhat.shape)\n",
    "# yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(yts, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## another method use self defined CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kares package use Tensorflow as backend\n",
    "# Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "# Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "# print(Xtr.shape)\n",
    "# print(ytr)\n",
    "# print(Xts.shape)\n",
    "# print(yts.shape)\n",
    "\n",
    "#change the features to improve accurancy\n",
    "rdim = Xtr.shape[0]\n",
    "sdim = Xts.shape[0]\n",
    "Xtr = Xtr.reshape(rdim, ydim, xdim)\n",
    "# Xtr = np.delete(Xtr, [19,20], 1)\n",
    "Xts = Xts.reshape(sdim, ydim, xdim)\n",
    "# Xts = np.delete(Xts, [19,20], 1)\n",
    "print(Xtr.shape)\n",
    "# print(type(Xtr))\n",
    "ydim = ydim - 0\n",
    "\n",
    "Xtr = Xtr.reshape(rdim, ydim, xdim, 1)\n",
    "Xts = Xts.reshape(sdim, ydim, xdim, 1)\n",
    "ytr_reshape = ytr.reshape(-1, 1)\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "ytr_hot = encoder.fit_transform(ytr_reshape)\n",
    "\n",
    "# cnt = Counter(Xtr)\n",
    "# print(\"Number of files in each category in TRAIN set:\")\n",
    "# for k in sorted(cnt.keys()):\n",
    "#     print(cnt[k])\n",
    "# print(Xtr.shape)\n",
    "# print(Xts.shape)\n",
    "in_shape = (ydim,xdim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# in_shape = (ydim,xdim,3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "#conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# normalize for each batch \n",
    "# model.add(BatchNormalization(input_shape=in_shape))\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (3,3), input_shape=in_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "model.add(Dropout(0.1)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, (3,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Layer 3\n",
    "# model.add(Convolution2D(conv_filters, (3, 3)))\n",
    "# model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid')) \n",
    "#model.add(Dense(256, activation='sigmoid')) \n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, for multi-class/multi-label problems use n output units \n",
    "# activation should be 'softmax' for multi-class / single-label output, 'sigmoid' for binary or multi-label tasks\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "# model.add(Dense(1,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# # CNN layers\n",
    "# # specify desired number of filters\n",
    "# n_filters = 16\n",
    "# input = Input(in_shape)\n",
    "\n",
    "# # The functional API allows to specify the predecessor in (brackets) after the new Layer function call\n",
    "# conv_layer1 = Convolution2D(n_filters, (10,4), activation='relu')(input)  # a vertical filter\n",
    "# conv_layer2 = Convolution2D(n_filters, (4,10), activation='relu')(input)  # a horizontal filter\n",
    "\n",
    "# # Pooling layers - equal sized\n",
    "# #maxpool1 = MaxPooling2D(pool_size=(2,2))(conv_layer1)\n",
    "# #maxpool2 = MaxPooling2D(pool_size=(2,2))(conv_layer2)\n",
    "\n",
    "# # ALTERNATIVE: Pooling layers - complementary to vertical/horizontal filter\n",
    "# #maxpool1 = MaxPooling2D(pool_size=(1,2))(conv_layer1)\n",
    "# #maxpool2 = MaxPooling2D(pool_size=(2,1))(conv_layer2)\n",
    "\n",
    "# # LARGER Pooling layers - complementary to vertical/horizontal filter\n",
    "# maxpool1 = MaxPooling2D(pool_size=(1,5))(conv_layer1)\n",
    "# maxpool2 = MaxPooling2D(pool_size=(5,1))(conv_layer2) # used 4,1 first\n",
    "\n",
    "# # Dropout for both layers\n",
    "# maxpool1 = Dropout(0.25)(maxpool1)\n",
    "# maxpool2 = Dropout(0.25)(maxpool2)\n",
    "\n",
    "# # we have to flatten the Pooling output in order to be concatenated\n",
    "# poolflat1 = Flatten()(maxpool1)\n",
    "# poolflat2 = Flatten()(maxpool2)\n",
    "\n",
    "# # Merge the 2 parallel pipelines\n",
    "# merged = concatenate([poolflat1, poolflat2],1)\n",
    "\n",
    "# full = Dense(256, activation='sigmoid')(merged)\n",
    "# output_layer = Dense(7, activation='softmax')(full)\n",
    "\n",
    "# # finally create the model\n",
    "# model = Model(input=input, output=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy' \n",
    "# loss = 'sparse_categorical_crossentropy' \n",
    "# learn_rate = 0.5\n",
    "# # Optimizer = Stochastic Gradient Descent\n",
    "# optimizer = optimizers.SGD(lr=learn_rate)\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "\n",
    "# YOU MAY RUN THIS CELL MULTIPLE TIMES TO CONTINUE TO TRAIN THE MODEL FURTHER\n",
    "\n",
    "# for how many epochs (iterations) to train\n",
    "epochs = 10\n",
    "\n",
    "# for training we need the \"1 hot encoded\" numeric classes of the ground truth\n",
    "# History = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)\n",
    "validation_percent = 0.1\n",
    "History = model.fit(Xtr, ytr_hot, validation_split=validation_percent, batch_size=32, epochs=epochs)\n",
    "\n",
    "# we keep the history of accuracies on training set\n",
    "# we append this to previous history in case we execute this cell multiple times\n",
    "if history is None:\n",
    "    history = History.history\n",
    "else:\n",
    "    for key in History.history.keys():\n",
    "        history[key].extend(History.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(Xts)\n",
    "test_pred = np.argmax(test_pred, axis=1)\n",
    "# test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yts, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(xtr.shape)\n",
    "# nin = xtr.shape[1]  # dimension of input data\n",
    "# nh = 100     # number of hidden units\n",
    "# nout = 8    # number of outputs = 10 since there are 10 classes\n",
    "# model = Sequential()\n",
    "# model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "# model.add(Dense(nout, activation='softmax', name='output'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class LossHistory(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "#         self.loss = []\n",
    "#         self.val_acc = []\n",
    " \n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         # TODO:  This is called at the end of each batch.  \n",
    "#         # Add the loss in logs.get('loss') to the loss list\n",
    "#         loss = logs.get('loss')\n",
    "#         self.loss.append(loss)\n",
    "        \n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         # TODO:  This is called at the end of each epoch.  \n",
    "#         # Add the test accuracy in logs.get('loss') to the val_acc list\n",
    "#         acc = logs.get('val_acc')\n",
    "#         self.val_acc.append(acc)\n",
    "\n",
    "# # Create an instance of the history callback\n",
    "# history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# opt = optimizers.Adam(lr=1e-5) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# model.fit(xtr, ytr, epochs=10, batch_size=batch_size, validation_data=(xts,yts), callbacks=[history_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
