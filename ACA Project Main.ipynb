{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ACA Project: Speech Emotion Detection\n",
    "    \n",
    "we used data mainly from two dataset (TESS and RAVDESS), and transform them into the consist format (21x85) for each audio wav, and reval them into 1D features to save in the csv file for further read. The source code for transformation part is in file ... the last two column data in csv file is the class and dataset label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 1787)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1777</th>\n",
       "      <th>1778</th>\n",
       "      <th>1779</th>\n",
       "      <th>1780</th>\n",
       "      <th>1781</th>\n",
       "      <th>1782</th>\n",
       "      <th>1783</th>\n",
       "      <th>1784</th>\n",
       "      <th>1785</th>\n",
       "      <th>1786</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-94.654073</td>\n",
       "      <td>-85.509316</td>\n",
       "      <td>-89.514151</td>\n",
       "      <td>-75.158692</td>\n",
       "      <td>-54.101195</td>\n",
       "      <td>16.318827</td>\n",
       "      <td>111.716281</td>\n",
       "      <td>114.988910</td>\n",
       "      <td>117.105902</td>\n",
       "      <td>100.701295</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.600350</td>\n",
       "      <td>44.900890</td>\n",
       "      <td>3.827597</td>\n",
       "      <td>5.621762</td>\n",
       "      <td>0.194946</td>\n",
       "      <td>6.518361</td>\n",
       "      <td>5.823970</td>\n",
       "      <td>10.533665</td>\n",
       "      <td>11.121295</td>\n",
       "      <td>9.023582</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-85.126966</td>\n",
       "      <td>-89.579010</td>\n",
       "      <td>-41.984810</td>\n",
       "      <td>49.378000</td>\n",
       "      <td>108.657153</td>\n",
       "      <td>109.867511</td>\n",
       "      <td>119.388176</td>\n",
       "      <td>116.645629</td>\n",
       "      <td>112.809038</td>\n",
       "      <td>109.047258</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-63.233798</td>\n",
       "      <td>20.978620</td>\n",
       "      <td>99.049608</td>\n",
       "      <td>95.757327</td>\n",
       "      <td>78.557140</td>\n",
       "      <td>84.435719</td>\n",
       "      <td>83.664807</td>\n",
       "      <td>68.762827</td>\n",
       "      <td>67.117229</td>\n",
       "      <td>62.209639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.972568</td>\n",
       "      <td>26.667942</td>\n",
       "      <td>0.224216</td>\n",
       "      <td>8.267901</td>\n",
       "      <td>22.654129</td>\n",
       "      <td>31.920317</td>\n",
       "      <td>18.691901</td>\n",
       "      <td>1.916780</td>\n",
       "      <td>5.505791</td>\n",
       "      <td>9.660367</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1          2          3           4           5     \\\n",
       "0 -94.654073 -85.509316 -89.514151 -75.158692  -54.101195   16.318827   \n",
       "1  84.600350  44.900890   3.827597   5.621762    0.194946    6.518361   \n",
       "2 -85.126966 -89.579010 -41.984810  49.378000  108.657153  109.867511   \n",
       "3 -63.233798  20.978620  99.049608  95.757327   78.557140   84.435719   \n",
       "4  87.972568  26.667942   0.224216   8.267901   22.654129   31.920317   \n",
       "\n",
       "         6           7           8           9     ...   1777  1778  1779  \\\n",
       "0  111.716281  114.988910  117.105902  100.701295  ...    1.0   1.0   1.0   \n",
       "1    5.823970   10.533665   11.121295    9.023582  ...    1.0   1.0   1.0   \n",
       "2  119.388176  116.645629  112.809038  109.047258  ...    1.0   1.0   1.0   \n",
       "3   83.664807   68.762827   67.117229   62.209639  ...    1.0   1.0   1.0   \n",
       "4   18.691901    1.916780    5.505791    9.660367  ...    1.0   1.0   1.0   \n",
       "\n",
       "   1780  1781  1782  1783  1784  1785  1786  \n",
       "0   1.0   1.0   1.0   1.0   1.0     0     0  \n",
       "1   1.0   1.0   1.0   1.0   1.0     0     0  \n",
       "2   1.0   1.0   1.0   1.0   1.0     0     0  \n",
       "3   1.0   1.0   1.0   1.0   1.0     0     0  \n",
       "4   1.0   1.0   1.0   1.0   1.0     0     0  \n",
       "\n",
       "[5 rows x 1787 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read data from xls file \n",
    "df = pd.read_csv('features.csv', header = None, na_values = '?', index_col = None)\n",
    "\n",
    "# filenames = ['features2.csv', 'features3.csv', 'features4.csv', 'features5.csv', 'features6.csv','test1.csv','test2.csv']\n",
    "# for filename in filenames:\n",
    "#     df1 = pd.read_csv(filename, header = None, na_values = '0', index_col = None)\n",
    "#     df = pd.concat([df,df1], ignore_index = True)\n",
    "data_column = np.shape(df)[1]\n",
    "data_row = np.shape(df)[0]\n",
    "print(np.shape(df))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 1785)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "# get the value \n",
    "y_raw = np.array(df[data_column-2])\n",
    "\n",
    "# list of 1D-features in the order of MFCC, Energy, Pitch\n",
    "X_raw = np.array(pd.DataFrame(df, columns = df.columns[0:(data_column-2)]))\n",
    "\n",
    "print(np.shape(X_raw))\n",
    "print(np.shape(y_raw))\n",
    "# print(X_raw)\n",
    "# print(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "(2618, 7)\n"
     ]
    }
   ],
   "source": [
    "# one hot coding \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "y_reshape = y_raw.reshape(-1, 1)\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "yhot = encoder.fit_transform(y_reshape)\n",
    "print(yhot)\n",
    "print(yhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2618, 1785)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "# normalize\n",
    "ydim = 21\n",
    "xdim = 85\n",
    "\n",
    "X = preprocessing.scale(X_raw)\n",
    "# print(X.shape)\n",
    "# scaler = preprocessing.standardScaler().fit(X_raw)\n",
    "# X = scaler.transform(X_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 1785)\n",
      "(2094,)\n",
      "(524, 1785)\n",
      "(524,)\n",
      "Number of files in each category in TRAIN set:\n",
      "318\n",
      "240\n",
      "308\n",
      "305\n",
      "310\n",
      "300\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "\n",
    "cnt = Counter(ytr)\n",
    "print(\"Number of files in each category in TRAIN set:\")\n",
    "for k in sorted(cnt.keys()):\n",
    "    print(cnt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use SVM model to predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.0073, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(probability=False,  kernel=\"rbf\", C=2.8, gamma=.0073,verbose=10)\n",
    "svc.fit(Xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.211832\n"
     ]
    }
   ],
   "source": [
    "yhat_ts = svc.predict(Xts)\n",
    "acc = np.mean(yhat_ts == yts)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from random import shuffle\n",
    "\n",
    "# def train_classifier(xtr, ytr):\n",
    "#     print(xtr.shape)\n",
    "#     print(ytr.shape)\n",
    "#     dataset = np.hstack([xtr,xtr])\n",
    "#     shuffle(dataset)\n",
    "#     xtr = dataset[0:136]\n",
    "#     ytr = dataset[137]\n",
    "#     clf = RandomForestClassifier(n_estimators=1000, max_depth = 100000, n_jobs = -1, oob_score = True, min_samples_split = 10)\n",
    "#     clf.fit(xtr, ytr)\n",
    "#     return clf\n",
    "\n",
    "\n",
    "# def test_classifier(classifier, xts, yts):\n",
    "#     y_pred = classifier.predict(xts)\n",
    "#     precision, recall, f1_score, _ = precision_recall_fscore_support(y_ts, y_pred)\n",
    "#     return [precision, recall, f1_score]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = train_classifier(xtr,ytr)\n",
    "# [precision, recall, f1_score] = test_classifier(clf,xts,yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Convolution2D, MaxPooling2D, Activation, merge\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 21, 85, 1)\n",
      "(524, 21, 85, 1)\n",
      "(2094, 63, 85, 3)\n",
      "(524, 63, 85, 3)\n"
     ]
    }
   ],
   "source": [
    "# try to use pre-trained deep learning network vgg16\n",
    "Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "print(Xtr.shape)\n",
    "print(Xts.shape)\n",
    "Xtr_1 = []\n",
    "Xts_1 = []\n",
    "Xtr_1 = Xtr.repeat(3, axis=1)\n",
    "Xts_1 = Xts.repeat(3, axis=1)\n",
    "Xtr_1 = Xtr_1.repeat(3, axis=3)\n",
    "Xts_1 = Xts_1.repeat(3, axis=3)\n",
    "print(Xtr_1.shape)\n",
    "print(Xts_1.shape)\n",
    "\n",
    "pre_trained = 'vgg16'\n",
    "\n",
    "# Load appropriate packages\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import decode_predictions, preprocess_input    \n",
    "\n",
    "input_shape = (ydim*3,xdim,3)\n",
    "base_model = applications.VGG16(weights='imagenet', include_top = False, input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 63, 85, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 63, 85, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 63, 85, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 31, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 31, 42, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 31, 42, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 15, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 15, 21, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 15, 21, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 15, 21, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 7, 10, 256)        0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 7, 10, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 7, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 7, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 14,977,345\n",
      "Trainable params: 262,657\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    model.add(layers)\n",
    "    \n",
    "for layers in model.layers:\n",
    "    layers.trainable = False\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 63, 85, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_1.shape)\n",
    "batch_size = 32\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator = train_datagen.flow(\n",
    "                        Xtr_1,ytr,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow(\n",
    "                        Xts_1,yts, \n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train_generator.n\n",
    "test_size = test_generator.n\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "steps_per_epoch =  train_size // batch_size\n",
    "validation_steps =  test_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "65/65 [==============================] - 156s - loss: -27.1110 - acc: 0.1149 - val_loss: -34.0644 - val_acc: 0.0996\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 154s - loss: -32.7714 - acc: 0.1147 - val_loss: -33.6345 - val_acc: 0.0874\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 153s - loss: -32.6977 - acc: 0.1157 - val_loss: -34.2826 - val_acc: 0.1016\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 161s - loss: -33.0473 - acc: 0.1128 - val_loss: -33.3753 - val_acc: 0.1220\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 156s - loss: -32.4306 - acc: 0.1149 - val_loss: -34.5094 - val_acc: 0.0955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132c0f9b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 5  # Number of epochs\n",
    "\n",
    "# Call the fit function\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nepochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## another method use self defined CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2094, 1785)\n",
      "[2 5 6 ..., 4 1 6]\n",
      "(524, 1785)\n",
      "(524,)\n"
     ]
    }
   ],
   "source": [
    "#kares package use Tensorflow as backend\n",
    "# Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "# Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, random_state=0)\n",
    "print(Xtr.shape)\n",
    "print(ytr)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "ytr_reshape = ytr.reshape(-1, 1)\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "ytr_hot = encoder.fit_transform(ytr_reshape)\n",
    "\n",
    "# cnt = Counter(Xtr)\n",
    "# print(\"Number of files in each category in TRAIN set:\")\n",
    "# for k in sorted(cnt.keys()):\n",
    "#     print(cnt[k])\n",
    "# print(Xtr.shape)\n",
    "# print(Xts.shape)\n",
    "# in_shape = Xtr.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irenecai/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(21, 85, 1...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/irenecai/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "#conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, input_shape=in_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid')) \n",
    "#model.add(Dense(256, activation='sigmoid')) \n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, for multi-class/multi-label problems use n output units \n",
    "# activation should be 'softmax' for multi-class / single-label output, 'sigmoid' for binary or multi-label tasks\n",
    "model.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 19, 83, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 9, 41, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 39, 16)         2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 19, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 912)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 200)               182600    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 1407      \n",
      "=================================================================\n",
      "Total params: 186,487\n",
      "Trainable params: 186,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy' \n",
    "learn_rate = 0.5\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = optimizers.SGD(lr=learn_rate)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1884 samples, validate on 210 samples\n",
      "Epoch 1/15\n",
      "1884/1884 [==============================] - 1s - loss: 2.0115 - acc: 0.2489 - val_loss: 1.4210 - val_acc: 0.4571\n",
      "Epoch 2/15\n",
      "1884/1884 [==============================] - 1s - loss: 1.1437 - acc: 0.5817 - val_loss: 1.0210 - val_acc: 0.6238\n",
      "Epoch 3/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.8679 - acc: 0.6842 - val_loss: 0.8560 - val_acc: 0.6667\n",
      "Epoch 4/15\n",
      "1884/1884 [==============================] - ETA: 0s - loss: 0.7152 - acc: 0.743 - 1s - loss: 0.7261 - acc: 0.7404 - val_loss: 0.8446 - val_acc: 0.7048\n",
      "Epoch 5/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.6116 - acc: 0.7797 - val_loss: 0.7135 - val_acc: 0.7429\n",
      "Epoch 6/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.5455 - acc: 0.8057 - val_loss: 0.8020 - val_acc: 0.7095\n",
      "Epoch 7/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.4891 - acc: 0.8254 - val_loss: 0.7017 - val_acc: 0.7524\n",
      "Epoch 8/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.4436 - acc: 0.8408 - val_loss: 0.7084 - val_acc: 0.7476\n",
      "Epoch 9/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.4518 - acc: 0.8360 - val_loss: 0.7022 - val_acc: 0.7286\n",
      "Epoch 10/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.4037 - acc: 0.8609 - val_loss: 0.7998 - val_acc: 0.7476\n",
      "Epoch 11/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.3719 - acc: 0.8747 - val_loss: 0.7297 - val_acc: 0.7571\n",
      "Epoch 12/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.3892 - acc: 0.8620 - val_loss: 0.5742 - val_acc: 0.8048\n",
      "Epoch 13/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.3447 - acc: 0.8785 - val_loss: 0.6061 - val_acc: 0.7810\n",
      "Epoch 14/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.3546 - acc: 0.8694 - val_loss: 0.8719 - val_acc: 0.6905\n",
      "Epoch 15/15\n",
      "1884/1884 [==============================] - 1s - loss: 0.3343 - acc: 0.8790 - val_loss: 0.7699 - val_acc: 0.7619\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "\n",
    "# YOU MAY RUN THIS CELL MULTIPLE TIMES TO CONTINUE TO TRAIN THE MODEL FURTHER\n",
    "\n",
    "# for how many epochs (iterations) to train\n",
    "epochs = 15\n",
    "\n",
    "# for training we need the \"1 hot encoded\" numeric classes of the ground truth\n",
    "# History = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)\n",
    "validation_percent = 0.1\n",
    "History = model.fit(Xtr, ytr_hot, validation_split=validation_percent, batch_size=32, epochs=epochs)\n",
    "\n",
    "# we keep the history of accuracies on training set\n",
    "# we append this to previous history in case we execute this cell multiple times\n",
    "if history is None:\n",
    "    history = History.history\n",
    "else:\n",
    "    for key in History.history.keys():\n",
    "        history[key].extend(History.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/524 [===============>..............] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(Xts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76717557251908397"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yts, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(xtr.shape)\n",
    "# nin = xtr.shape[1]  # dimension of input data\n",
    "# nh = 100     # number of hidden units\n",
    "# nout = 8    # number of outputs = 10 since there are 10 classes\n",
    "# model = Sequential()\n",
    "# model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "# model.add(Dense(nout, activation='softmax', name='output'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class LossHistory(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "#         self.loss = []\n",
    "#         self.val_acc = []\n",
    " \n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         # TODO:  This is called at the end of each batch.  \n",
    "#         # Add the loss in logs.get('loss') to the loss list\n",
    "#         loss = logs.get('loss')\n",
    "#         self.loss.append(loss)\n",
    "        \n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         # TODO:  This is called at the end of each epoch.  \n",
    "#         # Add the test accuracy in logs.get('loss') to the val_acc list\n",
    "#         acc = logs.get('val_acc')\n",
    "#         self.val_acc.append(acc)\n",
    "\n",
    "# # Create an instance of the history callback\n",
    "# history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# opt = optimizers.Adam(lr=1e-5) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# model.fit(xtr, ytr, epochs=10, batch_size=batch_size, validation_data=(xts,yts), callbacks=[history_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
