{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read data from xls file \n",
    "df = pd.read_csv('features1.csv', header = None, na_values = '?', index_col = None)\n",
    "\n",
    "filenames = ['features2.csv', 'features3.csv', 'features4.csv', 'features5.csv', 'features6.csv','test1.csv','test2.csv']\n",
    "for filename in filenames:\n",
    "    df1 = pd.read_csv(filename, header = None, na_values = '0', index_col = None)\n",
    "    df = pd.concat([df,df1], ignore_index = True)\n",
    "print(np.shape(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value \n",
    "y_raw = np.array(df[137])\n",
    "\n",
    "# list of 2D-features in the order of MFCC, Energy, Pitch\n",
    "X_raw = np.array(pd.DataFrame(df, columns = df.columns[0:136]))\n",
    "# print(np.shape(Xtrain))\n",
    "# print(Xtrain[135])\n",
    "\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# index = np.random.permutation(1612)\n",
    "# ntr = 1200\n",
    "# nts = 412\n",
    "# # # print(np.shape(X))\n",
    "\n",
    "# xtr = X[index[0:ntr]]\n",
    "# ytr = y[index[0:ntr]]\n",
    "# xts = X[index[ntr:ntr+nts]]\n",
    "# yts = y[index[ntr:ntr+nts]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot coding \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "y_reshape = y_raw.reshape(-1, 1)\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "yhot = encoder.fit_transform(y_reshape)\n",
    "print(yhot)\n",
    "print(yhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "# normalize\n",
    "N, ydim, xdim = X_raw.shape\n",
    "X_re = X_raw.reshape(N, xdim*ydim)\n",
    "X = preprocessing.scale(X_re)\n",
    "# scaler = preprocessing.standardScaler().fit(X_re)\n",
    "# X = scaler.transform(X_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rate = 0.2\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y_raw, test_size=test_rate, shuffle=True, random_state=0)\n",
    "\n",
    "cnt = Counter(ytr)\n",
    "print \"Number of files in each category in TRAIN set:\"\n",
    "for k in sorted(cnt.keys()):\n",
    "    print k, \":\", cnt[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use SVM model to predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc.fit(Xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat_ts = svc.predict(Xts)\n",
    "# acc = np.mean(yhat_ts == yts)\n",
    "# print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from random import shuffle\n",
    "\n",
    "# def train_classifier(xtr, ytr):\n",
    "#     print(xtr.shape)\n",
    "#     print(ytr.shape)\n",
    "#     dataset = np.hstack([xtr,xtr])\n",
    "#     shuffle(dataset)\n",
    "#     xtr = dataset[0:136]\n",
    "#     ytr = dataset[137]\n",
    "#     clf = RandomForestClassifier(n_estimators=1000, max_depth = 100000, n_jobs = -1, oob_score = True, min_samples_split = 10)\n",
    "#     clf.fit(xtr, ytr)\n",
    "#     return clf\n",
    "\n",
    "\n",
    "# def test_classifier(classifier, xts, yts):\n",
    "#     y_pred = classifier.predict(xts)\n",
    "#     precision, recall, f1_score, _ = precision_recall_fscore_support(y_ts, y_pred)\n",
    "#     return [precision, recall, f1_score]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = train_classifier(xtr,ytr)\n",
    "# [precision, recall, f1_score] = test_classifier(clf,xts,yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Convolution2D, MaxPooling2D, Activation, merge\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to use pre-trained deep learning network vgg16\n",
    "Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "Xtr_1 = np.array([Xtr,Xtr,Xtr])\n",
    "Xts_1 = np.zrray([Xts,Xts,Xts])\n",
    "\n",
    "pre_trained = 'vgg16'\n",
    "\n",
    "# Load appropriate packages\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import decode_predictions, preprocess_input    \n",
    "\n",
    "input_shape = (ydim,xdim,3)\n",
    "base_model = applications.VGG16(weights='imagenet', include_top = False, input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    model.add(layers)\n",
    "    \n",
    "for layers in model.layers:\n",
    "    layers.trainable = False\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator = train_datagen.flow(\n",
    "                        Xtr,ytr\n",
    "                        target_size=(ydim,xdim),\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow(\n",
    "                        Xts,yts\n",
    "                        target_size=(ydim,xdim), \n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train_generator.n\n",
    "test_size = test_generator.n\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "steps_per_epoch =  train_size // batch_size\n",
    "validation_steps =  test_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepochs = 5  # Number of epochs\n",
    "\n",
    "# Call the fit function\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nepochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## another method use self defined CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kares package use Tensorflow as backend\n",
    "Xtr = Xtr.reshape(Xtr.shape[0], ydim, xdim, 1)\n",
    "Xts = Xts.reshape(Xts.shape[0], ydim, xdim, 1)\n",
    "print(Xtr.shape)\n",
    "print(Xts.shape)\n",
    "in_shape = Xtr.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "#conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid')) \n",
    "#model.add(Dense(256, activation='sigmoid')) \n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, for multi-class/multi-label problems use n output units \n",
    "# activation should be 'softmax' for multi-class / single-label output, 'sigmoid' for binary or multi-label tasks\n",
    "model.add(Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy' \n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = optimizers.SGD(lr=learn_rate)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "\n",
    "# YOU MAY RUN THIS CELL MULTIPLE TIMES TO CONTINUE TO TRAIN THE MODEL FURTHER\n",
    "\n",
    "# for how many epochs (iterations) to train\n",
    "epochs = 15\n",
    "\n",
    "# for training we need the \"1 hot encoded\" numeric classes of the ground truth\n",
    "# History = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)\n",
    "validation_percent = 0.1\n",
    "History = model.fit(train_set, train_classes_1hot, validation_split=validation_percent, batch_size=32, nb_epoch=epochs)\n",
    "\n",
    "# we keep the history of accuracies on training set\n",
    "# we append this to previous history in case we execute this cell multiple times\n",
    "if history is None:\n",
    "    history = History.history\n",
    "else:\n",
    "    for key in History.history.keys():\n",
    "        history[key].extend(History.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtr.shape)\n",
    "# nin = xtr.shape[1]  # dimension of input data\n",
    "# nh = 100     # number of hidden units\n",
    "# nout = 8    # number of outputs = 10 since there are 10 classes\n",
    "# model = Sequential()\n",
    "# model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "# model.add(Dense(nout, activation='softmax', name='output'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class LossHistory(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "#         self.loss = []\n",
    "#         self.val_acc = []\n",
    " \n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         # TODO:  This is called at the end of each batch.  \n",
    "#         # Add the loss in logs.get('loss') to the loss list\n",
    "#         loss = logs.get('loss')\n",
    "#         self.loss.append(loss)\n",
    "        \n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         # TODO:  This is called at the end of each epoch.  \n",
    "#         # Add the test accuracy in logs.get('loss') to the val_acc list\n",
    "#         acc = logs.get('val_acc')\n",
    "#         self.val_acc.append(acc)\n",
    "\n",
    "# # Create an instance of the history callback\n",
    "# history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# opt = optimizers.Adam(lr=1e-5) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# model.fit(xtr, ytr, epochs=10, batch_size=batch_size, validation_data=(xts,yts), callbacks=[history_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
